import joblib
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report
from pipeline import process_pipeline
import os

# üìÅ D√©finition des chemins
MODEL_PATH = "deployment/final_model.pkl"
RAW_DATA_PATH = "data/raw/customer_churn_telecom_services.csv"
CLEAN_DATA_PATH = "data/processed/cleaned_data.csv"

def load_model():
    """Charge le mod√®le XGBoost depuis le fichier pickle."""
    try:
        model = joblib.load(MODEL_PATH)
        print("‚úÖ Mod√®le charg√© avec succ√®s !\n")
        return model
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement du mod√®le : {e}")
        exit(1)

def load_data():
    """V√©rifie et charge les donn√©es nettoy√©es, sinon les g√©n√®re avec process_pipeline."""
    if not os.path.exists(CLEAN_DATA_PATH):
        print("üîÑ Fichier nettoy√© introuvable, ex√©cution du pipeline de nettoyage...")
        process_pipeline(RAW_DATA_PATH, CLEAN_DATA_PATH)
    
    data = pd.read_csv(CLEAN_DATA_PATH)
    print(f"‚úÖ Donn√©es nettoy√©es charg√©es ({data.shape[0]} √©chantillons) !\n")
    
    X = data.drop(columns=['Churn'])  # Supposons que 'Churn' est la cible
    y = data['Churn']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)
    print(f"‚úÖ Donn√©es divis√©es : {X_test.shape[0]} √©chantillons pour le test\n")
    return X_test, y_test

def evaluate_model(model, X_test, y_test):
    """√âvalue les performances du mod√®le sur l'ensemble de test."""
    print("\n================ √âVALUATION DU MOD√àLE ================\n")
    
    # Pr√©dictions
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]
    
    # Calcul des m√©triques
    metrics = {
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1-Score": f1_score(y_test, y_pred),
        "AUC-ROC": roc_auc_score(y_test, y_proba)
    }
    
    # Affichage format√©
    for metric, value in metrics.items():
        print(f"üìä {metric: <12}: {value:.4f}")
    
    # Affichage de la matrice de confusion
    cm = confusion_matrix(y_test, y_pred)
    print("\nMatrice de Confusion :")
    print(cm, "\n")
    
    # Affichage du rapport de classification
    print("Rapport de Classification :")
    print(classification_report(y_test, y_pred))
    
    print("\n====================================================\n")

def main():
    """Ex√©cute l'√©valuation du mod√®le."""
    model = load_model()
    X_test, y_test = load_data()
    evaluate_model(model, X_test, y_test)

if __name__ == "__main__":
    main()
